{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"165px"},"toc_section_display":true,"toc_window_display":true},"colab":{"name":"pfrl_getting_start_part2.ipynb","provenance":[],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"b436f8459371407b92d3354f9527ffe8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bba1762ac00a4261bb45de9368cc3989","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_752922f71f4e4a95b3a8515907595a7e","IPY_MODEL_dc41d9f74f9e4fc2b0c2fbdb34aa2763"]}},"bba1762ac00a4261bb45de9368cc3989":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"752922f71f4e4a95b3a8515907595a7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1149f184172d44e58bbd4deec859ff18","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_219c555d546d48349e6e87bd42cce80f"}},"dc41d9f74f9e4fc2b0c2fbdb34aa2763":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5769cb56fb104ef98c2d43bf2d0e0a21","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [00:31&lt;00:00, 31.27it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e734f56a0a994455929636b6ea792638"}},"1149f184172d44e58bbd4deec859ff18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"219c555d546d48349e6e87bd42cce80f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5769cb56fb104ef98c2d43bf2d0e0a21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e734f56a0a994455929636b6ea792638":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70b9de0cc0e145e99ada6f2d8baba435":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2c2f2bdcd0ac4836bf5d1cbe248ac5fe","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8d1b021d073a46c488983168e8b5d7e0","IPY_MODEL_2b63e41f2cf84a9d9da779b8814625ee"]}},"2c2f2bdcd0ac4836bf5d1cbe248ac5fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8d1b021d073a46c488983168e8b5d7e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a2e09433bfbd42a299909b12443678ba","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3767ce23cb6a4738902f5d8b0a0db146"}},"2b63e41f2cf84a9d9da779b8814625ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_395847d030734e15963b0e90b5db6bcf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [1:35:01&lt;00:00,  5.70s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_509053d170d2411f95da252f963405a8"}},"a2e09433bfbd42a299909b12443678ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3767ce23cb6a4738902f5d8b0a0db146":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"395847d030734e15963b0e90b5db6bcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"509053d170d2411f95da252f963405a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b4f3038cb8244388ecb4f314774f14f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5cc320688c4a4744aeddf7264dba457e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2b6ef6bbb14d40bfb89a655c193228bd","IPY_MODEL_1b2dc418838e4e8282e87b996d375b2a"]}},"5cc320688c4a4744aeddf7264dba457e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b6ef6bbb14d40bfb89a655c193228bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1f6fdf40a1dc44f88634ee24ca05af12","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_56c6482ab89347bfa391267861bc94da"}},"1b2dc418838e4e8282e87b996d375b2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e5d8e25bccf84c8795d0b1ab696bb2a0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [25:07&lt;00:00,  1.51s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71b17236bbd64b8092605099e3e4f5b8"}},"1f6fdf40a1dc44f88634ee24ca05af12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"56c6482ab89347bfa391267861bc94da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e5d8e25bccf84c8795d0b1ab696bb2a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"71b17236bbd64b8092605099e3e4f5b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"GKdc2OIB9k37"},"source":["# 方策ベースの強化学習1 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9G2BmCN39nB1","executionInfo":{"status":"ok","timestamp":1611805939487,"user_tz":-540,"elapsed":58236,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"c5be7651-33c3-4114-d227-f4691ecaa70c"},"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:34:57.963393Z","start_time":"2021-01-26T19:34:57.708080Z"},"id":"_dH4553J9k4R","executionInfo":{"status":"ok","timestamp":1611806324993,"user_tz":-540,"elapsed":666,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["import sys"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ej5NZpnO9k4T"},"source":["### pfrlライブラリのパスへの追加 "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:34:58.550820Z","start_time":"2021-01-26T19:34:58.523907Z"},"id":"P-gcqW0W9k4U","executionInfo":{"status":"ok","timestamp":1611806332140,"user_tz":-540,"elapsed":562,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["sys.path.append(\"/content/gdrive/MyDrive/repos/pfrl\")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"38RMz9Tq9k4W"},"source":["### インポート "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:35:04.635544Z","start_time":"2021-01-26T19:34:59.349687Z"},"id":"nJ0yRDkt9k4W","executionInfo":{"status":"ok","timestamp":1611806363103,"user_tz":-540,"elapsed":29826,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["import pfrl\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:35:04.659473Z","start_time":"2021-01-26T19:35:04.645514Z"},"id":"uKsuXcW49k4Y","executionInfo":{"status":"ok","timestamp":1611806363109,"user_tz":-540,"elapsed":28890,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["from tqdm.notebook import tqdm\n","import cv2"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjayeSXq-8_-"},"source":["### 描画のための処理"]},{"cell_type":"markdown","metadata":{"id":"IpEZHTda-7wt"},"source":["[こちら](https://stackoverflow.com/a/61318224)あるいは[こちら](https://stackoverflow.com/a/61318224)を参考にした．"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sih8ReRz-tYV","executionInfo":{"status":"ok","timestamp":1611806412769,"user_tz":-540,"elapsed":44821,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"1cb33bc3-12c6-4022-d46f-f7331f517074"},"source":["!apt-get install -y xvfb x11-utils\r\n","!pip install -q gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  libxxf86dga1\n","Suggested packages:\n","  mesa-utils\n","The following NEW packages will be installed:\n","  libxxf86dga1 x11-utils xvfb\n","0 upgraded, 3 newly installed, 0 to remove and 13 not upgraded.\n","Need to get 993 kB of archives.\n","After this operation, 2,981 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n","Fetched 993 kB in 1s (1,323 kB/s)\n","Selecting previously unselected package libxxf86dga1:amd64.\n","(Reading database ... 146374 files and directories currently installed.)\n","Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n","Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n","Selecting previously unselected package x11-utils.\n","Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n","Unpacking x11-utils (7.7+3build1) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n","Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n","Setting up x11-utils (7.7+3build1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","\u001b[K     |████████████████████████████████| 542kB 5.2MB/s \n","\u001b[K     |████████████████████████████████| 450kB 7.7MB/s \n","\u001b[?25h  Building wheel for PyOpenGL-accelerate (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E2zx6_BD-40N","executionInfo":{"status":"ok","timestamp":1611806413548,"user_tz":-540,"elapsed":41980,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["import pyvirtualdisplay\r\n","_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\r\n","                                    size=(1400, 900))\r\n","_ = _display.start()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0DBVzVT49k4a"},"source":["### 離散行動・確率的方策 "]},{"cell_type":"markdown","metadata":{"id":"gPWd1AbZ9k4a"},"source":["離散行動の環境の例としてカートポールを利用し，方策ベースの学習手法としてREINFORCEを利用する．"]},{"cell_type":"code","metadata":{"id":"rJjX9NLB_moe","executionInfo":{"status":"ok","timestamp":1611806436479,"user_tz":-540,"elapsed":648,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["import gym"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:35:04.787136Z","start_time":"2021-01-26T19:35:04.672441Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"wXFrmR_89k4b","executionInfo":{"status":"ok","timestamp":1611770269497,"user_tz":-540,"elapsed":696,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"7f4b72e8-fc0c-46ea-f558-134f6ac1a0cd"},"source":["discrete_env = gym.make(\"CartPole-v0\")\n","print(\"observation space:\", discrete_env.observation_space)\n","print(\"action space:\", discrete_env.action_space)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n","action space: Discrete(2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"18PgEy939k4e"},"source":["#### 方策モデルの定義 "]},{"cell_type":"markdown","metadata":{"id":"EpP6n7vt9k4f"},"source":["REINFORCEでは関数近似器として，状態を入力して行動と行動確率（サンプル値とその尤度の値）が取得できる方策モデルがあればよい．そのためモデルは最終出力として`torch.distributions`のオブジェクトを出力する．`pfrl.policies.SoftmaxCategoricalHead`はカテゴリカル分布を表し，ロジットを入力として`torch.distributions`のクラスを出力する`nn.Module`のサブクラスである．ここで注意するのが`pfrl.action_value.ActionValue`はQ関数の最終行に記述したが，モデルがそのオブジェクト自体を返していた．この`pfrl.policies`のheadクラスは通常の`nn.Module`を継承したクラスと同じように，forward内でコールして利用する．しかし，返り値は`torch.distributions`のクラスである．"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T03:28:10.113254Z","start_time":"2021-01-18T03:28:10.074355Z"},"id":"tJ_EAw6Y9k4g","executionInfo":{"status":"ok","timestamp":1611770536213,"user_tz":-540,"elapsed":1244,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["class PolicyModel(nn.Module):\n","    def __init__(self, obs_size, n_actions):\n","        super().__init__()\n","        self.fc1 = nn.Linear(obs_size, 50)\n","        self.fc2 = nn.Linear(50, 50)\n","        self.fc3 = nn.Linear(50, n_actions)\n","        self.head = pfrl.policies.SoftmaxCategoricalHead()\n","        \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        logits = F.softmax(self.fc3(x), dim=1)  # カテゴリ次元をソフトマックス\n","        out = self.head(logits)\n","        return out"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T03:28:10.787447Z","start_time":"2021-01-18T03:28:10.736585Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"clmr-W-N9k4h","executionInfo":{"status":"ok","timestamp":1611770536575,"user_tz":-540,"elapsed":934,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"8fd007ff-b015-44af-c7df-694040b5d4e2"},"source":["obs_size = discrete_env.observation_space.low.size\n","print(\"observation size:\", obs_size)\n","n_actions = discrete_env.action_space.n\n","print(\"action size:\",n_actions)\n","policy_model = PolicyModel(obs_size, n_actions)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["observation size: 4\n","action size: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zSvVNdJc9k4i"},"source":["#### エージェントの定義"]},{"cell_type":"markdown","metadata":{"id":"cnj3XqZc9k4i"},"source":["on-policyの学習になるので，explolerは必要ない．各種ハイパーパラメータはprflのexamplesを参考にした．[参考](https://github.com/pfnet/pfrl/blob/master/examples/gym/train_reinforce_gym.py)"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T03:28:12.625534Z","start_time":"2021-01-18T03:28:12.597617Z"},"id":"6QC1fu539k4j","executionInfo":{"status":"ok","timestamp":1611770540235,"user_tz":-540,"elapsed":735,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["reinforce_opt = torch.optim.Adam(policy_model.parameters(), lr=1.e-3)\n","\n","gpu = -1\n","\n","beta = 1e-4\n","\n","batch_size = 10\n","\n","phi = lambda x: x.astype(np.float32, copy=False)\n","\n","reinforce_agent = pfrl.agents.REINFORCE(\n","    policy_model,\n","    reinforce_opt,\n","    gpu=gpu,\n","    beta=beta,\n","    batchsize=batch_size,\n","    max_grad_norm=1.0,\n","    phi=phi\n",")"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFi3dptS9k4k"},"source":["#### 学習のイテレーション "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T03:28:31.202869Z","start_time":"2021-01-18T03:28:13.620881Z"},"colab":{"base_uri":"https://localhost:8080/","height":609,"referenced_widgets":["b436f8459371407b92d3354f9527ffe8","bba1762ac00a4261bb45de9368cc3989","752922f71f4e4a95b3a8515907595a7e","dc41d9f74f9e4fc2b0c2fbdb34aa2763","1149f184172d44e58bbd4deec859ff18","219c555d546d48349e6e87bd42cce80f","5769cb56fb104ef98c2d43bf2d0e0a21","e734f56a0a994455929636b6ea792638"]},"id":"OSTulyui9k4l","executionInfo":{"status":"ok","timestamp":1611770575329,"user_tz":-540,"elapsed":33819,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"c7c97368-870e-40f7-84eb-6a56cf1619a4"},"source":["n_episodes = 1000  # エピソードの回数\n","max_episode_len = 200\n","for i in tqdm(range(1, n_episodes + 1)):\n","    obs = discrete_env.reset()  # 観測のリセット\n","    R = 0  # Return (sum ofrewards)\n","    t = 0  # time step\n","    while True:\n","        action = reinforce_agent.act(obs)\n","        obs, reward, done, _ = discrete_env.step(action)\n","        R += reward\n","        t += 1\n","        reset = t == max_episode_len\n","        reinforce_agent.observe(obs, reward, done, reset)\n","        if done or reset:\n","            break\n","    \n","    if i%50 == 0:\n","        print(\"episode:{}, return:{}\".format(i, R))\n","    if i%100 == 0:\n","        print(\"statistics:\", reinforce_agent.get_statistics())\n","        \n","print(\"Finshed\")"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b436f8459371407b92d3354f9527ffe8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["episode:50, return:14.0\n","episode:100, return:20.0\n","statistics: [('average_entropy', 0.6311372789739743)]\n","episode:150, return:37.0\n","episode:200, return:13.0\n","statistics: [('average_entropy', 0.6840180225643128)]\n","episode:250, return:13.0\n","episode:300, return:11.0\n","statistics: [('average_entropy', 0.6869980144349104)]\n","episode:350, return:26.0\n","episode:400, return:22.0\n","statistics: [('average_entropy', 0.6835209326775368)]\n","episode:450, return:20.0\n","episode:500, return:17.0\n","statistics: [('average_entropy', 0.6765118268849852)]\n","episode:550, return:33.0\n","episode:600, return:27.0\n","statistics: [('average_entropy', 0.6709195391967641)]\n","episode:650, return:42.0\n","episode:700, return:30.0\n","statistics: [('average_entropy', 0.6615873527748659)]\n","episode:750, return:44.0\n","episode:800, return:21.0\n","statistics: [('average_entropy', 0.6564494581582058)]\n","episode:850, return:31.0\n","episode:900, return:14.0\n","statistics: [('average_entropy', 0.6516458360013909)]\n","episode:950, return:46.0\n","episode:1000, return:21.0\n","statistics: [('average_entropy', 0.6516350633758491)]\n","\n","Finshed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yprT-J-U9k4m"},"source":["Q関数と比べてreturnが安定していない．"]},{"cell_type":"markdown","metadata":{"id":"hThCv0-H9k4n"},"source":["#### 学習結果の可視化 "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-17T16:06:16.428422Z","start_time":"2021-01-17T16:06:10.999546Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"eS98xUri9k4o","executionInfo":{"status":"ok","timestamp":1611770819810,"user_tz":-540,"elapsed":1318,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"f6ba3263-f0d8-4513-9d00-840dde18f8b3"},"source":["fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n","out = cv2.VideoWriter('/content/gdrive/MyDrive/rl_tutorial/movies/pfrl_tutorial_reinforce_result.mp4', fourcc, 10, (600, 400))\n","\n","max_episode_len = 200\n","with reinforce_agent.eval_mode():\n","    \n","    obs = discrete_env.reset()  # 観測のリセット\n","    R = 0  # Return (sum ofrewards)\n","    t = 0  # time step\n","    \n","    while True:\n","        action = reinforce_agent.act(obs)\n","        obs, reward, done, _ = discrete_env.step(action)\n","        R += reward\n","        t += 1\n","        reset = t == max_episode_len\n","        # 画像として取得，保存\n","        frame = discrete_env.render(mode=\"rgb_array\")\n","        out.write(frame[:,:,::-1])       \n","        \n","        #reinforce_agent.observe(obs, reward, done, reset)\n","        if done or reset:\n","            break\n","            \n","discrete_env.close()\n","out.release()\n","\n","print(\"episode length:\", t)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["episode length: 41\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qSpm72149k4r"},"source":["### 連続行動・確率的方策 "]},{"cell_type":"markdown","metadata":{"id":"dMJxcpOv9k4s"},"source":["連続行動の環境としてペンデュラムを利用し，方策ベースの学習手法としてA2Cを利用する．"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:35:31.424854Z","start_time":"2021-01-26T19:35:31.405913Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"MsG0jiwi9k4s","executionInfo":{"status":"ok","timestamp":1611806472576,"user_tz":-540,"elapsed":512,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"90fcd58c-e473-48ea-cbea-311feff6b0af"},"source":["concrete_env = gym.make(\"Pendulum-v0\")\n","print(\"observation space:\", concrete_env.observation_space)\n","print(\"action space:\", concrete_env.action_space)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["observation space: Box(-8.0, 8.0, (3,), float32)\n","action space: Box(-2.0, 2.0, (1,), float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q3AJ09gQ9k4u"},"source":["#### 環境の可視化 "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-19T07:19:11.398344Z","start_time":"2021-01-19T07:19:08.343519Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"W6EuhVo_9k4u","executionInfo":{"status":"ok","timestamp":1611770869418,"user_tz":-540,"elapsed":766,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"f33e8760-7150-4f41-b2f5-15a7a9a4f217"},"source":["concrete_env.reset()\n","array = concrete_env.render(mode=\"rgb_array\")\n","concrete_env.close()\n","array.shape"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(500, 500, 3)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T03:28:50.179137Z","start_time":"2021-01-18T03:28:41.621018Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"WzBkjTTl9k4w","executionInfo":{"status":"ok","timestamp":1611806490846,"user_tz":-540,"elapsed":3599,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"a04f9614-9271-4cf1-8368-3f1af5b3e1e4"},"source":["fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n","out = cv2.VideoWriter('/content/gdrive/MyDrive/rl_tutorial/movies/pfrl_tutorial_a2c.mp4', fourcc, 10, (500, 500))\n","\n","max_episode_len = 200\n","\n","R = 0  # Return (sum ofrewards)\n","t = 0  # time step\n","concrete_env.reset()\n","\n","while True:\n","    action = concrete_env.action_space.sample()\n","    obs, reward, done, _ = concrete_env.step(action)\n","    R += reward\n","    t += 1\n","    reset = t == max_episode_len\n","    # 画像として取得，保存\n","    frame = concrete_env.render(mode=\"rgb_array\")\n","    out.write(frame[:,:,::-1])\n","    \n","    if done or reset:\n","        break\n","        \n","concrete_env.close()\n","out.release()\n","\n","print(\"episode length:\", t)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["episode length: 200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_Efb1PdZ9k4x"},"source":["#### モデルの定義 "]},{"cell_type":"markdown","metadata":{"id":"KpU0wMM49k4y"},"source":["A2Cのモデルは方策モデルと状態の価値関数が一体化したものである．一般的には状態を入力として途中から，行動空間の次元数の確率変数を出力する`torch.distributions`のクラスのオブジェクトを返すブランチと\n","状態に対して一つの勝ちを返すブランチの二つに枝分かれさせる．"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:35:34.833733Z","start_time":"2021-01-26T19:35:34.759936Z"},"id":"2H3NjhUw9k4z","executionInfo":{"status":"ok","timestamp":1611806495569,"user_tz":-540,"elapsed":1112,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["class PolicyValueModel(nn.Module):\n","    def __init__(self, obs_size, action_dim, action_low, action_high):\n","        super().__init__()\n","        self.fc1 = nn.Linear(obs_size, 32)\n","        self.bn1 = nn.BatchNorm1d(32)\n","        \n","        self.fc2 = nn.Linear(32, 128)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        \n","        self.fc3 = nn.Linear(128, 256)\n","        self.bn3 = nn.BatchNorm1d(256)\n","        \n","        self.br1_fc1 = nn.Linear(256, 50)\n","        self.br1_bn1 = nn.BatchNorm1d(50)\n","        \n","        self.br1_fc2 = nn.Linear(50, action_dim)\n","        self.br1_bn2 = nn.BatchNorm1d(action_dim)\n","        self.policy_bound = pfrl.nn.BoundByTanh(action_low, action_high)\n","        self.policy_head = pfrl.policies.GaussianHeadWithFixedCovariance(0.1)\n","        \n","        self.br2_fc1 = nn.Linear(256, 50)\n","        self.br2_bn1 = nn.BatchNorm1d(50)\n","        self.value_head = nn.Linear(50, 1)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.fc1(x)))\n","        x = F.relu(self.bn2(self.fc2(x)))\n","        branch = F.relu(self.bn3(self.fc3(x)))\n","        \n","        policy_x = F.relu(self.br1_bn1(self.br1_fc1(branch)))\n","        policy_x = self.policy_bound(self.br1_bn2(self.br1_fc2(policy_x)))\n","        out_policy = self.policy_head(policy_x)\n","        \n","        \n","        value_x = F.relu(self.br2_bn1(self.br2_fc1(branch))) \n","        out_value = self.value_head(value_x)  # exampleの実装では最終層の活性化関数は無い\n","        \n","        return out_policy, out_value"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:35:35.227681Z","start_time":"2021-01-26T19:35:35.144902Z"},"id":"uFFY0CWZ9k40","executionInfo":{"status":"ok","timestamp":1611806499164,"user_tz":-540,"elapsed":614,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["obs_size = concrete_env.observation_space.low.size\n","action_dim = concrete_env.action_space.low.size\n","action_low = concrete_env.action_space.low.item()\n","action_high = concrete_env.action_space.high.item()\n","\n","policy_value_model = PolicyValueModel(obs_size, action_dim, action_low, action_high)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kj9Y5sBx9k42"},"source":["#### マルチプロセス用のバッチ環境 "]},{"cell_type":"markdown","metadata":{"id":"nkxHwUxK9k44"},"source":["multiprocessingやjoblibでは，マルチプロセスで実行する関数をリストとして渡す．そこでここでも環境の作成を関数で行う．ここでその関数は引数が無いようにすることに注意．そのためランダムシード等を引数で与える場合は，\n","関数をラップするか`functiools.partial`を利用する．[参考](https://github.com/pfnet/pfrl/blob/master/examples/atari/train_a2c_ale.py)"]},{"cell_type":"markdown","metadata":{"id":"iapri9qA9k46"},"source":["ここでwindowsでjupyterを使う人にとって致命的なバグがあり，multiprocessingで利用する関数は.pyファイルに書いておきjupyterからimportしなければならない（[参考](https://stackoverflow.com/questions/45719956/python-multiprocessing-in-jupyter-on-windows-attributeerror-cant-get-attribut)）．以下の関数は`tutorial_make_env_ver1.py`に書かれている．<- joblibでラップするべき？"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:35:40.011877Z","start_time":"2021-01-26T19:35:39.991940Z"},"id":"Y6AEcSLR9k47","executionInfo":{"status":"ok","timestamp":1611771053455,"user_tz":-540,"elapsed":1170,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["sys.path.append(\"/content/gdrive/MyDrive/rl_tutorial\")\r\n","from tutorial_make_env_ver1 import make_concrete_env"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-19T07:27:39.025118Z","start_time":"2021-01-19T07:27:38.949331Z"},"id":"cuT79Pwb9k47","outputId":"81c01d3c-fb28-4d29-8ee2-8d7b242dfae4"},"source":["\"\"\"\n","def make_concrete_env():\n","    one_concrete_env = gym.make(\"Pendulum-v0\")\n","    return one_concrete_env\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef make_concrete_env():\\n    one_concrete_env = gym.make(\"Pendulum-v0\")\\n    return one_concrete_env\\n'"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"2LChL4W_9k48"},"source":["ミニバッチの環境(マルチプロセッシング可能)を作成するには，`pfrl.envs.MultiprocessVectorEnv`クラスを利用する．これは環境のリストをラップし，それぞれをマルチプロセスで動かす．インターフェースとしては既存と同じ`step`と`reset`が利用できるが，返り値は全てミニバッチとして返ってくる．ただし問題なのは，ミニバッチのサイズ(環境の数)がプロセス数と必ず一致することである([参考](https://github.com/pfnet/pfrl/blob/master/pfrl/envs/multiprocess_vector_env.py))．これはミニバッチ数を大きくしたいときに問題となる．もしそれを解決したいなら，`joblib`の`Pararel`等で実装しなおす必要がありそう．"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-26T19:36:09.826098Z","start_time":"2021-01-26T19:35:44.592625Z"},"id":"qMK82Ot19k49","executionInfo":{"status":"ok","timestamp":1611772224564,"user_tz":-540,"elapsed":1458,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["process_number = 64\n","batch_concrete_env = pfrl.envs.MultiprocessVectorEnv([make_concrete_env for i in range(process_number)])"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WemEICk9k4-"},"source":["#### エージェントの定義 "]},{"cell_type":"markdown","metadata":{"id":"WxO_KoLr9k4_"},"source":["ハイパーパラメータはこちらによる．([参考](https://github.com/pfnet/pfrl/blob/master/examples/atari/train_a2c_ale.py))"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-22T07:37:14.824055Z","start_time":"2021-01-22T07:37:14.809095Z"},"id":"zoIfqBPv9k4_","executionInfo":{"status":"ok","timestamp":1611772227649,"user_tz":-540,"elapsed":854,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["#a2c_opt = pfrl.optimizers.RMSpropEpsInsideSqrt(\n","#        policy_value_model.parameters(),\n","#        lr=1e-3,\n","#        eps=1e-5,\n","#        alpha=0.99,\n","#    )\n","\n","a2c_opt =  torch.optim.Adam(policy_value_model.parameters(), lr=3e-4)\n","\n","gamma = 0.99\n","\n","update_steps = 5\n","\n","phi = lambda x: x.astype(np.float32, copy=False)\n","\n","use_gae = True\n","\n","tau = 0.95\n","\n","max_grad_norm = 40\n","\n","gpu = -1\n","\n","num_processes = process_number\n","\n","\n","a2c_agent = pfrl.agents.A2C(\n","    policy_value_model,\n","    a2c_opt,\n","    gamma=gamma,\n","    gpu=gpu,\n","    num_processes=process_number,\n","    update_steps=update_steps,\n","    phi=phi,\n","    use_gae=use_gae,\n","    tau=tau,\n","    max_grad_norm=max_grad_norm,\n",")"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QodYB8cX9k5B"},"source":["#### 学習のイテレーション"]},{"cell_type":"markdown","metadata":{"id":"TNKX7tqQ9k5C"},"source":["A2Cでは，ミニバッチごとにact, observeを行うので(obs, reward等が全てミニバッチ)ミニバッチに対応する`batch_act`,`batch_observe`を利用する．今回は少し強引だが，バッチ環境のうち一つでもdoneしても，学習を続行させるプログラムにしている．"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-22T09:05:30.044480Z","start_time":"2021-01-22T07:37:14.828044Z"},"scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":664,"referenced_widgets":["70b9de0cc0e145e99ada6f2d8baba435","2c2f2bdcd0ac4836bf5d1cbe248ac5fe","8d1b021d073a46c488983168e8b5d7e0","2b63e41f2cf84a9d9da779b8814625ee","a2e09433bfbd42a299909b12443678ba","3767ce23cb6a4738902f5d8b0a0db146","395847d030734e15963b0e90b5db6bcf","509053d170d2411f95da252f963405a8"]},"id":"4ozHao9A9k5D","executionInfo":{"status":"ok","timestamp":1611777936622,"user_tz":-540,"elapsed":4502928,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"a6397cce-500e-4d8d-f2a3-b84f3ded5839"},"source":["n_episodes = 1000  # エピソードの回数\n","max_episode_len = 200\n","for i in tqdm(range(1, n_episodes + 1)):\n","    \n","    obss = batch_concrete_env.reset()  # 観測のリセット\n","    R = np.zeros((process_number,))  # Return (sum ofrewards)\n","    t = 0  # time step\n","    while True:\n","        actions = a2c_agent.batch_act(obss)\n","        obss, rewards, dones, _ = batch_concrete_env.step(actions)\n","        \n","        R += rewards\n","        t += 1\n","        \n","        resets = np.array([t == max_episode_len]*process_number)\n","        a2c_agent.batch_observe(obss, rewards, dones, resets)\n","        if resets[0]:\n","            break\n","    \n","    if i%50 == 0:\n","        print(\"episode:{}, mean_return:{}, t:{}\".format(i, R.mean(), t))\n","    if i%100 == 0:\n","        print(\"statistics:\", a2c_agent.get_statistics())\n","print(\"Finshed\")\n","batch_concrete_env.close()  # バッチで利用する場合は明示的にクローズ"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70b9de0cc0e145e99ada6f2d8baba435","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/content/gdrive/MyDrive/repos/pfrl/pfrl/agents/a2c.py:268: UserWarning: A2C currently does not support resetting an env without reaching a terminal state during training. When receiving True in batch_reset, A2C considers it as True in batch_done instead.\n","  \"A2C currently does not support resetting an env without reaching a\"\n"],"name":"stderr"},{"output_type":"stream","text":["episode:50, mean_return:-1175.7830742616425, t:200\n","episode:100, mean_return:-1120.450325845581, t:200\n","statistics: [('average_actor', 458.906357498217), ('average_value', 6930.20006086006), ('average_entropy', -0.8674942965115855)]\n","episode:150, mean_return:-941.6859109575046, t:200\n","episode:200, mean_return:-1355.9904992415554, t:200\n","statistics: [('average_actor', 247.40206476980526), ('average_value', 6061.153092431643), ('average_entropy', -0.8833512419577557)]\n","episode:250, mean_return:-1105.8471729179723, t:200\n","episode:300, mean_return:-1097.0844730020835, t:200\n","statistics: [('average_actor', 605.8405753089817), ('average_value', 7735.061089967709), ('average_entropy', -0.8836410913779691)]\n","episode:350, mean_return:-1384.2258563084406, t:200\n","episode:400, mean_return:-1517.232408550611, t:200\n","statistics: [('average_actor', 542.8257823745596), ('average_value', 7255.166254936951), ('average_entropy', -0.8836463895412688)]\n","episode:450, mean_return:-1699.341645664927, t:200\n","episode:500, mean_return:-1061.0932929627893, t:200\n","statistics: [('average_actor', 845.4871613835314), ('average_value', 8303.36042489443), ('average_entropy', -0.8836464863864999)]\n","episode:550, mean_return:-1307.6580162916177, t:200\n","episode:600, mean_return:-1447.2685282976272, t:200\n","statistics: [('average_actor', 369.759506966962), ('average_value', 7324.141014501439), ('average_entropy', -0.8836464881567362)]\n","episode:650, mean_return:-1368.4228204684596, t:200\n","episode:700, mean_return:-1513.0169625831031, t:200\n","statistics: [('average_actor', 698.505976495779), ('average_value', 8127.200690685524), ('average_entropy', -0.8836464881890949)]\n","episode:750, mean_return:-1555.9562337235282, t:200\n","episode:800, mean_return:-1167.018732196038, t:200\n","statistics: [('average_actor', 725.3224184320583), ('average_value', 8935.437353024892), ('average_entropy', -0.8836464881896419)]\n","episode:850, mean_return:-1150.2661592451889, t:200\n","episode:900, mean_return:-1435.9325517908578, t:200\n","statistics: [('average_actor', 751.3225480139835), ('average_value', 9373.36459039895), ('average_entropy', -0.8836464881896419)]\n","episode:950, mean_return:-1560.4925725890616, t:200\n","episode:1000, mean_return:-1512.382893717715, t:200\n","statistics: [('average_actor', 159.99620366671223), ('average_value', 10286.453884728224), ('average_entropy', -0.8836464881896419)]\n","\n","Finshed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pGY7yR859k5G"},"source":["#### 学習結果の可視化"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-19T08:30:18.430104Z","start_time":"2021-01-19T08:26:10.988Z"},"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"jDeJVfIm9k5G","executionInfo":{"status":"error","timestamp":1611805854622,"user_tz":-540,"elapsed":1815,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"12887441-1fbe-47ec-c4b2-f81b4d8f6350"},"source":["fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n","out = cv2.VideoWriter('movies/tutorial/pfrl_tutorial_a2c_result.mp4', fourcc, 10, (500, 500))\n","\n","max_episode_len = 200\n","with a2c_agent.eval_mode():\n","    \n","    obs = concrete_env.reset()  # 観測のリセット\n","    R = 0  # Return (sum ofrewards)\n","    t = 0  # time step\n","    \n","    while True:\n","        action = a2c_agent.act(obs)\n","        obs, reward, done, _ = concrete_env.step(action)\n","        R += reward\n","        t += 1\n","        reset = t == max_episode_len\n","        # 画像として取得，保存\n","        frame = concrete_env.render(mode=\"rgb_array\")\n","        out.write(frame[:,:,::-1])       \n","        \n","        #a2c_agent.observe(obs, reward, done, reset)\n","        if done or reset:\n","            break\n","            \n","concrete_env.close()\n","out.release()\n","\n","print(\"episode length:\", t)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-eae61ff18fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfourcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoWriter_fourcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movies/tutorial/pfrl_tutorial_a2c_result.mp4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfourcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_episode_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma2c_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"xTuSlrP49k5H"},"source":["### 連続行動・決定的方策"]},{"cell_type":"markdown","metadata":{"id":"Q_oRMVZj9k5I"},"source":["連続行動の環境としてペンデュラムを利用し，学習手法としてDDPGを利用する．"]},{"cell_type":"markdown","metadata":{"id":"t-1eAYBn9k5J"},"source":["#### 方策モデルの定義"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T23:26:58.082324Z","start_time":"2021-01-18T23:26:57.978544Z"},"id":"DheS-xnn9k5L","executionInfo":{"status":"ok","timestamp":1611806516059,"user_tz":-540,"elapsed":572,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["class DPolicy(nn.Module):\n","    def __init__(self, obs_size, action_dim, action_low=-2., action_high=2.):\n","        super().__init__()\n","        self.fc1 = nn.Linear(obs_size, 50)\n","        self.fc2 = nn.Linear(50, 50)\n","        self.fc3 = nn.Linear(50, action_dim)\n","        self.policy_bound = pfrl.nn.BoundByTanh(action_low, action_high)\n","        self.head = pfrl.policies.DeterministicHead()\n","        \n","        \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.policy_bound(self.fc3(x))\n","        out = self.head(x)\n","        return out"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T23:32:26.838156Z","start_time":"2021-01-18T23:32:26.804238Z"},"id":"Z6WkkNI99k5N","executionInfo":{"status":"ok","timestamp":1611806517425,"user_tz":-540,"elapsed":623,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["obs_size = concrete_env.observation_space.low.size\n","action_dim = concrete_env.action_space.low.size\n","action_low = concrete_env.action_space.low.item()\n","action_high = concrete_env.action_space.high.item()\n","\n","dpolicy_model = DPolicy(obs_size, action_dim, action_low, action_high)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GC02jFOc9k5O"},"source":["#### 価値関数の定義  "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T23:32:29.252688Z","start_time":"2021-01-18T23:32:29.204817Z"},"id":"MuEi3Uqw9k5O","executionInfo":{"status":"ok","timestamp":1611806534822,"user_tz":-540,"elapsed":602,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["class QFunc(nn.Module):\n","    def __init__(self, obs_size, action_dim):\n","        super().__init__()\n","        self.concat_obs_action = pfrl.nn.ConcatObsAndAction()\n","        self.fc1 = nn.Linear(obs_size+action_dim, 50)\n","        self.fc2 = nn.Linear(50, 50)\n","        self.fc3 = nn.Linear(50, 1)\n","        \n","    def forward(self, x):\n","        x = self.concat_obs_action(x)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)  # 最後は活性化関数は必要は無い\n","        return x"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-18T23:32:29.930879Z","start_time":"2021-01-18T23:32:29.903965Z"},"id":"hgwJ1M6E9k5P","executionInfo":{"status":"ok","timestamp":1611806541449,"user_tz":-540,"elapsed":588,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["q_func = QFunc(obs_size, action_dim)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQ7wjZVG9k5P"},"source":["####  エージェントの定義"]},{"cell_type":"markdown","metadata":{"id":"z30NxJGR9k5Q"},"source":["ハイパーパラメータはこちらを参考とした．([参考](https://github.com/pfnet/pfrl/blob/master/examples/mujoco/reproduction/ddpg/train_ddpg.py))"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-19T00:47:49.522352Z","start_time":"2021-01-19T00:47:49.327862Z"},"id":"fS-eR8119k5Q","executionInfo":{"status":"ok","timestamp":1611806546910,"user_tz":-540,"elapsed":737,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}}},"source":["opt_a = torch.optim.Adam(dpolicy_model.parameters())\n","opt_c = torch.optim.Adam(q_func.parameters())\n","\n","rbuf = pfrl.replay_buffers.ReplayBuffer(10 ** 6)\n","\n","explorer = pfrl.explorers.AdditiveGaussian(\n","    scale=0.1, low=concrete_env.action_space.low, high=concrete_env.action_space.high\n",")\n","\n","def burnin_action_func():\n","    \"\"\"Select random actions until model is updated one or more times.\"\"\"\n","    return np.random.uniform(concrete_env.action_space.low, concrete_env.action_space.high).astype(np.float32)\n","\n","gpu = -1\n","\n","phi = lambda x: x.astype(np.float32, copy=False)\n","\n","ddpg_agent = pfrl.agents.DDPG(\n","    dpolicy_model,\n","    q_func,\n","    opt_a,\n","    opt_c,\n","    rbuf,\n","    phi=phi,\n","    gamma=0.99,\n","    explorer=explorer,\n","    replay_start_size=10000,\n","    target_update_method=\"soft\",\n","    target_update_interval=1,\n","    update_interval=1,\n","    soft_update_tau=5e-3,\n","    n_times_update=1,\n","    gpu=gpu,\n","    minibatch_size=100,\n","    burnin_action_func=burnin_action_func,\n",")"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-waUvu1r9k5R"},"source":["#### 学習のイテレーション "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-19T01:14:00.272024Z","start_time":"2021-01-19T00:47:51.266681Z"},"colab":{"base_uri":"https://localhost:8080/","height":609,"referenced_widgets":["2b4f3038cb8244388ecb4f314774f14f","5cc320688c4a4744aeddf7264dba457e","2b6ef6bbb14d40bfb89a655c193228bd","1b2dc418838e4e8282e87b996d375b2a","1f6fdf40a1dc44f88634ee24ca05af12","56c6482ab89347bfa391267861bc94da","e5d8e25bccf84c8795d0b1ab696bb2a0","71b17236bbd64b8092605099e3e4f5b8"]},"id":"9O8bqgwt9k5S","executionInfo":{"status":"ok","timestamp":1611808062038,"user_tz":-540,"elapsed":1508055,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"cf98d08d-c212-4f66-8441-6be563e73ebe"},"source":["n_episodes = 1000  # エピソードの回数\n","max_episode_len = 200\n","for i in tqdm(range(1, n_episodes + 1)):\n","    obs = concrete_env.reset()  # 観測のリセット\n","    R = 0  # Return (sum ofrewards)\n","    t = 0  # time step\n","    while True:\n","        action = ddpg_agent.act(obs)\n","        obs, reward, done, _ = concrete_env.step(action)\n","        R += reward\n","        t += 1\n","        reset = t == max_episode_len\n","        ddpg_agent.observe(obs, reward, done, reset)\n","        if done or reset:\n","            break\n","    \n","    if i%50 == 0:\n","        print(\"episode:{}, return:{}\".format(i, R))\n","    if i%100 == 0:\n","        print(\"statistics:\", ddpg_agent.get_statistics())\n","        \n","print(\"Finshed\")"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b4f3038cb8244388ecb4f314774f14f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["episode:50, return:-1161.9399052654878\n","episode:100, return:-122.12085454592133\n","statistics: [('average_q', -77.58938), ('average_actor_loss', 74.69236209869385), ('average_critic_loss', 43.86319679498673), ('n_updates', 10001)]\n","episode:150, return:-129.42891704137153\n","episode:200, return:-247.81231782509693\n","statistics: [('average_q', -6.0514655), ('average_actor_loss', 4.3349128979444504), ('average_critic_loss', 19.149380059242247), ('n_updates', 30001)]\n","episode:250, return:-120.67450189419914\n","episode:300, return:-121.61343329644191\n","statistics: [('average_q', -11.601319), ('average_actor_loss', 13.167333605289459), ('average_critic_loss', 11.36274651169777), ('n_updates', 50001)]\n","episode:350, return:-122.65157773489214\n","episode:400, return:-123.05538413803674\n","statistics: [('average_q', -11.975549), ('average_actor_loss', 12.316498830318451), ('average_critic_loss', 14.24978187441826), ('n_updates', 70001)]\n","episode:450, return:-140.23478000737543\n","episode:500, return:-242.18539948868784\n","statistics: [('average_q', -5.1478457), ('average_actor_loss', 4.041350655257702), ('average_critic_loss', 7.128876060843468), ('n_updates', 90001)]\n","episode:550, return:-262.90028337587154\n","episode:600, return:-12.567134651710937\n","statistics: [('average_q', -2.4592462), ('average_actor_loss', 4.399116532579065), ('average_critic_loss', 9.886009595692158), ('n_updates', 110001)]\n","episode:650, return:-252.37929870829132\n","episode:700, return:-16.937600628178032\n","statistics: [('average_q', 1.0327256), ('average_actor_loss', 1.0104740245454014), ('average_critic_loss', 14.939433839917182), ('n_updates', 130001)]\n","episode:750, return:-251.17154289076754\n","episode:800, return:-253.23535453078395\n","statistics: [('average_q', -7.107918), ('average_actor_loss', 8.28044778585434), ('average_critic_loss', 7.955107318162918), ('n_updates', 150001)]\n","episode:850, return:-12.238718741229519\n","episode:900, return:-257.2545512234226\n","statistics: [('average_q', -8.743162), ('average_actor_loss', 10.320471091270447), ('average_critic_loss', 7.523478559851647), ('n_updates', 170001)]\n","episode:950, return:-136.63214355008475\n","episode:1000, return:-120.97810318464062\n","statistics: [('average_q', 28.104727), ('average_actor_loss', -28.87697227478027), ('average_critic_loss', 15.747458861470223), ('n_updates', 190001)]\n","\n","Finshed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kZUsqFOp9k5T"},"source":["#### 学習結果の可視化 "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-01-19T01:14:07.493718Z","start_time":"2021-01-19T01:14:00.276013Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"5Tjd5B2k9k5U","executionInfo":{"status":"ok","timestamp":1611808078050,"user_tz":-540,"elapsed":2171,"user":{"displayName":"Naoto Asami","photoUrl":"","userId":"02738946460850472304"}},"outputId":"b73ee038-f8e8-457a-f27e-6d6b4c7fcf43"},"source":["fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n","out = cv2.VideoWriter('movies/tutorial/pfrl_tutorial_ddpg_result.mp4', fourcc, 10, (500, 500))\n","\n","max_episode_len = 200\n","with ddpg_agent.eval_mode():\n","    \n","    obs = concrete_env.reset()  # 観測のリセット\n","    R = 0  # Return (sum ofrewards)\n","    t = 0  # time step\n","    \n","    while True:\n","        action = ddpg_agent.act(obs)\n","        obs, reward, done, _ = concrete_env.step(action)\n","        R += reward\n","        t += 1\n","        reset = t == max_episode_len\n","        # 画像として取得，保存\n","        frame = concrete_env.render(mode=\"rgb_array\")\n","        out.write(frame[:,:,::-1])       \n","        \n","        #a2c_agent.observe(obs, reward, done, reset)\n","        if done or reset:\n","            break\n","            \n","concrete_env.close()\n","out.release()\n","\n","print(\"episode length:\", t)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["episode length: 200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jjFPnVk09k5V"},"source":["a2cよりDDPGの方がうまく学習できている．"]},{"cell_type":"markdown","metadata":{"id":"-O-mc12K9k5X"},"source":["今回はa2cで連続行動・確率方策がうまく行かなかったので，次回はsoft actor criticで試してみる．"]}]}